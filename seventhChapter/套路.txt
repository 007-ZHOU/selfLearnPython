1.创建项目
    scrapy startproject 项目名称

2.进入项目
    cd 项目名称
3.创建爬虫
    scrapy genspider 名字 域名

4. 可能需要修改start_urls，修改成你要抓取的拉个页面

5.运行scrapy 
    scrapy crawl 项目名称

5.对数据解析解析 ，在spider里面的parse（response）方法中进行解析

    def parse(self，response):
        response.text 拿到页面源代码
        response.xpath()
        response.css()

        解析数据的时候 需要注意，默认xpath()返回的是selecctor对象
        想要数据必须使用extract()提取数据
        extract()返回列表
        extract_first()返回一个数据

        yield 返回数据 把数据交给pipline来进行持久化存储 


6. 在pipline中完成数据的存储
    class 类名():
        def process_item(self,item,spider):
            item 数据
            spider 爬虫
            return item  必须要return 东西，否则下一个管道收不到数据


7.设置  settings.py 文件将pipeline 进行生效设置
ITEM_PIPELINES开启
